# build_index.py
# import torch
import os
import time
from concurrent.futures import ThreadPoolExecutor

from llama_index.core import Settings, StorageContext, Document, PropertyGraphIndex
from llama_index.core.node_parser import SimpleNodeParser
from tqdm import tqdm

from db.database import SessionLocal
from db.models import Note, Blog, Photo
from rag_engine import config
from rag_engine.formatters.formatter import format_note, format_blog, format_photo
from rag_engine.graph_rag.simple_graph_store_with_schema import SimpleGraphStoreWithSchema
from rag_engine.providers import get_embed_model, get_extractor_llm


def batch_process_documents(docs, kg_index, parser, batch_size=4):
    """æ‰¹é‡å¤„ç†æ–‡æ¡£"""
    total_batches = (len(docs) + batch_size - 1) // batch_size
    batch_bar = tqdm(total=total_batches, desc="âš¡ æ‰¹é‡å¤„ç†", unit="batch")

    with ThreadPoolExecutor(max_workers=2) as executor:  # é™åˆ¶å·¥ä½œçº¿ç¨‹æ•°
        futures = []
        for i in range(0, len(docs), batch_size):
            batch = docs[i:i + batch_size]
            futures.append(executor.submit(
                process_document_batch, batch, kg_index, parser
            ))

        for future in futures:
            future.result()
            batch_bar.update(1)
            # log_gpu_memory()  # è®°å½•å†…å­˜ä½¿ç”¨

    batch_bar.close()


def process_document_batch(batch, kg_index, parser):
    """å¤„ç†å•æ‰¹æ–‡æ¡£"""
    nodes = []
    for doc in batch:
        nodes.extend(parser.get_nodes_from_documents([doc]))
    kg_index.insert_nodes(nodes)


def build_graph_index(max_triplets_per_chunk: int = 10):
    start_time = time.time()

    # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
    os.makedirs(config.GRAPH_DB_DIR, exist_ok=True)
    print(f"ğŸ“ å­˜å‚¨ç›®å½•: {config.GRAPH_DB_DIR}")

    # è®¾ç½®å…¨å±€é…ç½®
    Settings.embed_model = get_embed_model()
    extractor_llm = get_extractor_llm()
    print(f"ğŸ”§ ä½¿ç”¨æå–æ¨¡å‹: {config.EXTRACTION_MODEL}")

    # åˆå§‹åŒ–å›¾è°±å­˜å‚¨
    graph_store = SimpleGraphStoreWithSchema()
    storage_context = StorageContext.from_defaults(
        graph_store=graph_store
    )

    # ä½¿ç”¨å¤§å—èŠ‚ç‚¹è§£æå™¨ä¿æŒå®ä½“å®Œæ•´
    parser = SimpleNodeParser.from_defaults(
        chunk_size=4096,
        chunk_overlap=0
    )

    # åˆ›å»ºçŸ¥è¯†å›¾è°±ç´¢å¼•
    print("ğŸ› ï¸ åˆ›å»ºçŸ¥è¯†å›¾è°±ç´¢å¼•...")
    kg_index = PropertyGraphIndex(
        [],
        storage_context=storage_context,
        llm=extractor_llm
    )

    # ä»æ•°æ®åº“è·å–æ•°æ®
    print("ğŸ“¥ ä»æ•°æ®åº“åŠ è½½æ•°æ®...")
    with SessionLocal() as session:
        notes = session.query(Note).all()
        blogs = session.query(Blog).all()
        photos = session.query(Photo).all()

    print(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {len(notes)}ç¯‡ç¬”è®°, {len(blogs)}ç¯‡åšå®¢, {len(photos)}å¼ ç…§ç‰‡")

    # å‡†å¤‡æ‰€æœ‰æ–‡æ¡£
    all_docs = []
    for note in notes:
        all_docs.append(Document(text=format_note(note)))
    for blog in blogs:
        all_docs.append(Document(text=format_blog(blog)))
    for photo in photos:
        all_docs.append(Document(text=format_photo(photo)))

    print(f"ğŸ“š å…± {len(all_docs)} ä¸ªæ–‡æ¡£éœ€è¦å¤„ç†")

    # æ‰¹é‡å¤„ç†æ–‡æ¡£
    batch_process_documents(all_docs, kg_index, parser, batch_size=4)

    # ç»Ÿä¸€æŒä¹…åŒ–
    print("ğŸ’¾ æŒä¹…åŒ–ç´¢å¼•æ•°æ®...")
    storage_context.persist(persist_dir=config.GRAPH_DB_DIR)

    duration = time.time() - start_time
    print(f"âœ… GraphRAG ç´¢å¼•æ„å»ºå®Œæˆ! è€—æ—¶: {duration // 60:.0f}åˆ† {duration % 60:.0f}ç§’")
    print(f"ğŸ“¦ å­˜å‚¨ä½ç½®: {config.GRAPH_DB_DIR}")
    print(f"ğŸ“¦ ç´¢å¼•ID: {kg_index.index_id}")
    return kg_index
